{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZcF0Za98xMW"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "import uuid\n",
        "import random\n",
        "import threading\n",
        "import sys\n",
        "import os\n",
        "import socket\n",
        "import time\n",
        "import uuid\n",
        "import xmlrpc.client\n",
        "import xmlrpc.server as rpcserver\n",
        "import traceback\n",
        "import json\n",
        "from collections.abc import Callable\n",
        "import socketserver\n",
        "import asyncio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEN3_ViFZf2m"
      },
      "outputs": [],
      "source": [
        "NODE_COUNT = 5\n",
        "SHARD_SIZE = 3\n",
        "AGGREGATOR_COUNT = 3\n",
        "FIRST_CLIENT = 26\n",
        "FOLD_INDEX = 3\n",
        "nodes = {}\n",
        "print_lock = threading.RLock()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU3q_BtQ5pSs"
      },
      "outputs": [],
      "source": [
        "class Voting:\n",
        "  def __init__(self):\n",
        "    self.commits = {}\n",
        "    self.reveals = {}\n",
        "    self.res = 0\n",
        "\n",
        "  def commit(self, node_id, h):\n",
        "    self.commits[node_id] = h\n",
        "\n",
        "  def reveal(self, node_id, n):\n",
        "    self.reveals[node_id] = n\n",
        "\n",
        "  def verify_hash(self, h, n):\n",
        "    return h == hashlib.sha256(str(n).encode()).hexdigest()\n",
        "\n",
        "  def verify_voting(self):\n",
        "    for k in self.reveals.keys():\n",
        "      if k not in self.commits:\n",
        "        return False\n",
        "      if not self.verify_hash(self.commits[k], self.reveals[k]):\n",
        "        return False\n",
        "    shared_random_number = 0\n",
        "    for revealed_number in self.reveals.values():\n",
        "        shared_random_number ^= revealed_number\n",
        "    self.res = shared_random_number\n",
        "    return True\n",
        "\n",
        "  def result(self):\n",
        "    return self.res\n",
        "\n",
        "  def clear(self):\n",
        "    self.commits = {}\n",
        "    self.reveals = {}\n",
        "    self.res = 0\n",
        "\n",
        "class Elector:\n",
        "  def __init__(self, id: str):\n",
        "    self.id = id\n",
        "    self.random_number = 0\n",
        "    self.commitment = ''\n",
        "    self.voting = Voting()\n",
        "    self.sorting = []\n",
        "    self.election_pool = []\n",
        "\n",
        "  def start_election(self):\n",
        "    self.election_pool = list(nodes.keys())\n",
        "    self.start_voting()\n",
        "\n",
        "  def start_voting(self):\n",
        "    self.random_number = random.randint(0, len(self.election_pool) - 1)\n",
        "    self.commitment = hashlib.sha256(str(self.random_number).encode()).hexdigest()\n",
        "\n",
        "  def store_voting(self):\n",
        "    bit_count = len(bin(len(self.election_pool))[2:])\n",
        "    sum = 0\n",
        "    for i in range(0, bit_count):\n",
        "      sum += math.pow(2, i)\n",
        "    index = 0\n",
        "    if sum == 0:\n",
        "      index = 0\n",
        "    else:\n",
        "      index = math.floor(self.voting.result() / sum * max(0, len(self.election_pool) - 1))\n",
        "    self.sorting.append(self.election_pool[index])\n",
        "    self.election_pool.pop(index)\n",
        "    self.voting.clear()\n",
        "\n",
        "  def commit(self):\n",
        "    return self.commitment\n",
        "\n",
        "  def reveal(self):\n",
        "    return self.random_number\n",
        "\n",
        "  def start(self):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeovZ3MBBAv1"
      },
      "outputs": [],
      "source": [
        "class Learner:\n",
        "  def __init__(self, my_address, my_dataset, model_arch):\n",
        "    self.my_address = my_address\n",
        "    self.all_initial_shares = {}\n",
        "    self.initial_share = []\n",
        "    self.model = []\n",
        "    self.model_arch = model_arch\n",
        "    self.dataset = my_dataset\n",
        "\n",
        "  def prepare_initial_share(self):\n",
        "    rand_layers = []\n",
        "    for layer in self.model_arch.layers:\n",
        "      weights = layer.get_weights()\n",
        "      rand_layer = []\n",
        "      for sublayer in weights:\n",
        "        rand_layer.append(sublayer)\n",
        "      rand_layers.append(rand_layer)\n",
        "    self.initial_share = rand_layers\n",
        "    self.all_initial_shares[self.my_address] = self.initial_share\n",
        "\n",
        "  def commit_other_initial_shares(self, node_id, share):\n",
        "    self.all_initial_shares[node_id] = share\n",
        "\n",
        "  def prepare_initial_model(self, list_of_concat):\n",
        "    layers = []\n",
        "    counter = 0\n",
        "    for i in range(0, len(self.all_initial_shares[self.my_address])):\n",
        "      weights = self.all_initial_shares[self.my_address][i]\n",
        "      sublayers = []\n",
        "      for j in range(0, len(weights)):\n",
        "        sublayer = weights[j]\n",
        "        sublayers.append(self.all_initial_shares[list_of_concat[counter]][i][j])\n",
        "        counter = (counter + 1) % len(list_of_concat)\n",
        "      layers.append(sublayers)\n",
        "    self.model = layers\n",
        "    for i in range(0, len(layers)):\n",
        "      self.model_arch.layers[i].set_weights(layers[i])\n",
        "\n",
        "  def clear(self):\n",
        "    self.all_initial_shares = {}\n",
        "    self.initial_share = []\n",
        "\n",
        "  def initial_train(self, X, y_one_hot):\n",
        "    self.model_arch.fit(X, y_one_hot, epochs=80, batch_size=64, validation_split=0.3)\n",
        "\n",
        "  def train(self):\n",
        "    self.model_arch.fit(self.dataset[0], self.dataset[1], validation_data=(self.dataset[2], self.dataset[3]), epochs=40, batch_size=64, validation_split=0.3, verbose=0)\n",
        "    layers = []\n",
        "    for layer in self.model_arch.layers:\n",
        "      weights = layer.get_weights()\n",
        "      sublayers = []\n",
        "      for sublayer in weights:\n",
        "        sublayers.append(sublayer)\n",
        "      layers.append(sublayers)\n",
        "    self.model = layers\n",
        "\n",
        "  def commit_final_model(self, model):\n",
        "    self.model = model\n",
        "    for i in range(0, len(model)):\n",
        "      self.model_arch.layers[i].set_weights(model[i])\n",
        "\n",
        "  def prepare_for_transfer(self):\n",
        "    for layer in self.model_arch.layers:\n",
        "      if isinstance(layer, Conv1D) or isinstance(layer, MaxPooling1D):\n",
        "        layer.trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS-PERUuU7Go"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "RANDOM_TRANSACTION_COUNT = 2   # How many transactions to generate for each event\n",
        "RANDOM_TRANSACTION_AMOUNT_MAX = 500  # Maximum amount in a random transaction\n",
        "RANDOM_TRANSACTION_AMOUNT_MIN = 10   # Minimum amount in a random transaction\n",
        "\n",
        "# Transaction: A statement of money transfer from a sender to a receiver.\n",
        "class Transaction:\n",
        "    def __init__(self, sender_address: str = '', payload: Dict = {}):\n",
        "        self.sender_address = sender_address       # ip:port of sender\n",
        "        self.payload = payload                     # payload\n",
        "\n",
        "    def from_dict(self, my_dict):\n",
        "        for key in my_dict:\n",
        "            setattr(self, key, my_dict[key])\n",
        "        return self\n",
        "\n",
        "# Event: Represents an event in the hashgraph.\n",
        "class Event:\n",
        "    def __init__(self,\n",
        "                 owner: str = '',\n",
        "                 signature: str = '',\n",
        "                 self_parent_hash: str = '',\n",
        "                 other_parent_hash: str = '',\n",
        "                 timestamp: float = 0,\n",
        "                 transactions: List[Transaction] = [],\n",
        "                 round: int = 0,\n",
        "                 is_witness: bool = False,\n",
        "                 is_famous: bool = False,\n",
        "                 is_fame_decided: bool = False,\n",
        "                 round_received: int = 0,\n",
        "                 consensus_timestamp: Optional[float] = 0):\n",
        "        self.owner = owner\n",
        "        self.signature = signature\n",
        "        self.self_parent_hash = self_parent_hash\n",
        "        self.other_parent_hash = other_parent_hash\n",
        "        self.timestamp = timestamp\n",
        "        self.transactions = transactions\n",
        "        self.round = round\n",
        "        self.is_witness = is_witness\n",
        "        self.is_famous = is_famous\n",
        "        self.is_fame_decided = is_fame_decided\n",
        "        self.round_received = round_received\n",
        "        self.consensus_timestamp = consensus_timestamp if consensus_timestamp else 0\n",
        "        self.latency = None  # To be set later\n",
        "\n",
        "    def from_dict(self, my_dict):\n",
        "        for key in my_dict:\n",
        "            setattr(self, key, my_dict[key])\n",
        "        for i in range(0, len(self.transactions)):\n",
        "            self.transactions[i] = Transaction().from_dict(self.transactions[i])\n",
        "        return self\n",
        "\n",
        "# SyncEventsDTO: Data Transfer Object for SyncAllEvents function\n",
        "class SyncEventsDTO:\n",
        "    def __init__(self, sender_address: str = '', missing_events: Dict[str, List[Event]] = {}):\n",
        "        self.sender_address = sender_address  # address of the node who made the call\n",
        "        self.missing_events = missing_events  # map of addresses to events of those addresses that are missing on the remotely called node\n",
        "\n",
        "    def from_dict(self, my_dict):\n",
        "        for key in my_dict:\n",
        "            setattr(self, key, my_dict[key])\n",
        "        for key in self.missing_events:\n",
        "            for i in range(0, len(self.missing_events[key])):\n",
        "                self.missing_events[key][i] = Event().from_dict(self.missing_events[key][i])\n",
        "        return self\n",
        "\n",
        "class HgNode:\n",
        "    def __init__(self, initial_hashgraph: Dict[str, List[Event]], address: str, my_dataset: List[float], model_arch):\n",
        "        self.lock = threading.RLock()\n",
        "        self.address = address  # ip:port of the peer\n",
        "        self.hashgraph: Dict[str, List[Event]] = initial_hashgraph  # local copy of hashgraph\n",
        "        self.events: Dict[str, Event] = {}\n",
        "        self.witnesses: Dict[str, Dict[int, Event]] = {}\n",
        "        self.first_round_of_fame_undecided: Dict[str, int] = {}\n",
        "        self.first_event_of_not_consensus_index: Dict[str, int] = {}\n",
        "        self.consensus_events: List[Event] = []\n",
        "        self.transaction_buffer: List[Transaction] = []\n",
        "        self.see_dp_memory: Dict[str, Dict[str, bool]] = {}  # p.Signature -> q.Signature -> bool\n",
        "        self.on_commit_received: Callable = None\n",
        "        self.on_reveal_received: Callable = None\n",
        "        self.on_voting_next_round: Callable = None\n",
        "        self.elector = Elector(address)\n",
        "        self.voting_round_dones: Dict[str, bool] = {}\n",
        "        self.shared_order: List[str] = []\n",
        "        self.learner = Learner(address, my_dataset, model_arch)\n",
        "\n",
        "    def ping(self):\n",
        "        return \"pong\"\n",
        "\n",
        "    def voting_commit(self, sender_address: str, h: str, success: Optional[bool] = None):\n",
        "      self.elector.voting.commit(sender_address, h)\n",
        "      self.on_commit_received()\n",
        "      success = True\n",
        "      return True\n",
        "\n",
        "    def voting_reveal(self, sender_address: str, n: int, success: Optional[bool] = None):\n",
        "      self.elector.voting.reveal(sender_address, n)\n",
        "      self.on_reveal_received()\n",
        "      success = True\n",
        "      return True\n",
        "\n",
        "    def voting_done(self, sender_address: str, success: Optional[bool] = None):\n",
        "      self.voting_round_dones[sender_address] = True\n",
        "      self.on_voting_next_round()\n",
        "      success = True\n",
        "      return True\n",
        "\n",
        "    # GetNumberOfMissingEvents: Node A calls Node B to learn which events B does not know and A knows.\n",
        "    def get_number_of_missing_events(self, num_events_already_known: Dict[str, int], num_events_to_send: Optional[Dict[str, int]] = None) -> None:\n",
        "        if num_events_to_send is None:\n",
        "            num_events_to_send = {}\n",
        "        with self.lock:\n",
        "            for addr in self.hashgraph:\n",
        "                num_events_to_send[addr] = num_events_already_known.get(addr, 0) - len(self.hashgraph[addr])\n",
        "        return num_events_to_send\n",
        "\n",
        "    # SyncAllEvents: Node A first calls GetNumberOfMissingEvents on B, and then sends the missing events in this function\n",
        "    def sync_all_events(self, events: SyncEventsDTO, success: Optional[bool] = None) -> bool:\n",
        "        try:\n",
        "\n",
        "          # events = SyncEventsDTO().from_dict(events)\n",
        "\n",
        "          with self.lock:\n",
        "              other_peer_addresses = [addr for addr in self.hashgraph if addr != self.address]\n",
        "              #transactions = self.generate_transactions(RANDOM_TRANSACTION_COUNT, RANDOM_TRANSACTION_AMOUNT_MAX, RANDOM_TRANSACTION_AMOUNT_MIN, other_peer_addresses)\n",
        "\n",
        "              # Add the missing events to my local hashgraph\n",
        "              for addr, missing_events in events.missing_events.items():\n",
        "                for missing_event in missing_events:\n",
        "                  if missing_event.signature not in self.events:\n",
        "                    self.hashgraph.setdefault(addr, []).append(missing_event)\n",
        "                    self.events[missing_event.signature] = missing_event\n",
        "                    if missing_event.is_witness:\n",
        "                      self.witnesses.setdefault(missing_event.owner, {})[missing_event.round] = missing_event\n",
        "\n",
        "              # Store the transactions temporarily, and reset the global buffer\n",
        "              transactions = []\n",
        "              transactions.extend(self.transaction_buffer)\n",
        "              self.transaction_buffer = []\n",
        "\n",
        "              # Create random signature\n",
        "              signature = str(uuid.uuid4())\n",
        "\n",
        "              # Assign parents\n",
        "              new_events_self_parent = self.hashgraph[self.address][-1]\n",
        "              new_events_other_parent = self.hashgraph[events.sender_address][-1]\n",
        "\n",
        "              if len(transactions) > 0:\n",
        "                # Create event\n",
        "                new_event = Event(\n",
        "                  owner=self.address,\n",
        "                  signature=signature,\n",
        "                  self_parent_hash=new_events_self_parent.signature,\n",
        "                  other_parent_hash=new_events_other_parent.signature,\n",
        "                  timestamp=datetime.now().timestamp(),\n",
        "                  transactions=transactions\n",
        "                )\n",
        "\n",
        "                # Find the round & witness of new event\n",
        "                self.divide_rounds(new_event)\n",
        "\n",
        "                # Update local arrays\n",
        "                if new_event.is_witness:\n",
        "                  self.witnesses.setdefault(new_event.owner, {})[new_event.round] = new_event\n",
        "                self.events[new_event.signature] = new_event\n",
        "                self.hashgraph.setdefault(self.address, []).append(new_event)\n",
        "\n",
        "              # Decide fame on fame-undecided witnesses\n",
        "              self.decide_fame()\n",
        "\n",
        "              # Arrive to consensus on order of events\n",
        "              self.find_order()\n",
        "\n",
        "              if success is not None:\n",
        "                success = True\n",
        "              return True\n",
        "        except Exception as e:\n",
        "          with print_lock:\n",
        "            print(traceback.format_exc())\n",
        "          raise e\n",
        "\n",
        "    # DivideRounds: Calculates the round of a new event\n",
        "    def divide_rounds(self, event: Event) -> None:\n",
        "        self_parent = self.events.get(event.self_parent_hash)\n",
        "        other_parent = self.events.get(event.other_parent_hash)\n",
        "        if not self_parent or not other_parent:\n",
        "            with print_lock:\n",
        "              print(f\"Parents were not ok: (self: {self_parent is not None}, other: {other_parent is not None})\")\n",
        "            return\n",
        "\n",
        "        # Round of this event is at least the round of its parents\n",
        "        r = max(self_parent.round, other_parent.round)\n",
        "\n",
        "        # Get round r witnesses\n",
        "        witnesses = self.find_witnesses_of_a_round(r)\n",
        "\n",
        "        # Count strongly seen witnesses in round r\n",
        "        strongly_seen_witness_count = 0\n",
        "        for w in witnesses.values():\n",
        "            if self.strongly_see(event, w):\n",
        "                strongly_seen_witness_count += 1\n",
        "\n",
        "        # Check supermajority\n",
        "        if float(strongly_seen_witness_count) > (2.0 * len(self.hashgraph) / 3.0):\n",
        "            event.round = r + 1\n",
        "        else:\n",
        "            event.round = r\n",
        "\n",
        "        # Check if this new event is a witness\n",
        "        if event.round > self_parent.round:\n",
        "            event.is_witness = True\n",
        "\n",
        "    # DecideFame: Decides if a witness is famous or not\n",
        "    def decide_fame(self) -> None:\n",
        "        # Get the last witnesses that do not have a decided fame\n",
        "        fame_undecided_witnesses: List[Event] = []\n",
        "        for addr in self.hashgraph:\n",
        "            for round_num, witness in self.witnesses.get(addr, {}).items():\n",
        "                if round_num >= self.first_round_of_fame_undecided.get(addr, 0):\n",
        "                    fame_undecided_witnesses.append(witness)\n",
        "\n",
        "        for e in fame_undecided_witnesses:\n",
        "            # Get all witnesses that have greater rounds\n",
        "            witnesses_with_greater_rounds: List[Event] = []\n",
        "            for addr in self.hashgraph:\n",
        "                for round_num, witness in self.witnesses.get(addr, {}).items():\n",
        "                    if round_num > e.round:\n",
        "                        witnesses_with_greater_rounds.append(witness)\n",
        "\n",
        "            for w in witnesses_with_greater_rounds:\n",
        "                # Find witnesses of prior round\n",
        "                witnesses_of_round = self.find_witnesses_of_a_round(w.round - 1)\n",
        "\n",
        "                # Choose the strongly seen ones\n",
        "                strongly_seen_witnesses_of_round: List[Event] = []\n",
        "                for wr in witnesses_of_round.values():\n",
        "                    if self.strongly_see(w, wr):\n",
        "                        strongly_seen_witnesses_of_round.append(wr)\n",
        "\n",
        "                # Count votes\n",
        "                votes = [self.see(voter, e) for voter in strongly_seen_witnesses_of_round]\n",
        "                majority = sum(1 if vote else -1 for vote in votes)\n",
        "                true_votes = votes.count(True)\n",
        "                false_votes = votes.count(False)\n",
        "                majority_vote = majority >= 0\n",
        "                super_majority_threshold = 2.0 * len(self.hashgraph) / 3.0\n",
        "                if ((majority_vote and float(true_votes) > super_majority_threshold) or\n",
        "                    (not majority_vote and float(false_votes) > super_majority_threshold)):\n",
        "                    e.is_famous = majority_vote\n",
        "                    e.is_fame_decided = True\n",
        "                    self.first_round_of_fame_undecided[e.owner] = e.round + 1\n",
        "                    break\n",
        "\n",
        "    # FindOrder: Arrive at a consensus on the order of events\n",
        "    def find_order(self) -> None:\n",
        "        # Find events\n",
        "        non_consensus_events: List[Event] = []\n",
        "        for addr in self.hashgraph:\n",
        "            index = self.first_event_of_not_consensus_index.get(addr, 0)\n",
        "            non_consensus_events.extend(self.hashgraph[addr][index:])\n",
        "\n",
        "        for e in non_consensus_events:\n",
        "            # First & Third conditions: find a valid round number\n",
        "            r = self.first_round_of_fame_undecided.get(e.owner, 0)\n",
        "            for round_num in self.first_round_of_fame_undecided.values():\n",
        "                if round_num < r:\n",
        "                    r = round_num\n",
        "\n",
        "            witnesses = self.find_witnesses_of_a_round(r)\n",
        "            if witnesses:\n",
        "                # Second condition: make sure x is seen by all famous witnesses\n",
        "                cond_met = all(not w.is_famous or self.see(w, e) for w in witnesses.values())\n",
        "                if cond_met:\n",
        "                    # Construct consensus set\n",
        "                    s: List[Event] = []\n",
        "                    for w in witnesses.values():\n",
        "                        z = w\n",
        "                        while not is_initial(z):\n",
        "                            if z.round < e.round:\n",
        "                                break\n",
        "                            if self.see(z, e) and not self.see(self.events.get(z.self_parent_hash), e):\n",
        "                                s.append(z)\n",
        "                            z = self.events.get(z.self_parent_hash)\n",
        "                            if z is None:\n",
        "                                break\n",
        "\n",
        "                    if s:\n",
        "                        e.round_received = r\n",
        "                        # Take median\n",
        "                        sorted_timestamps = sorted([se.timestamp for se in s])\n",
        "                        median_timestamp = sorted_timestamps[len(sorted_timestamps) // 2]\n",
        "                        e.consensus_timestamp = median_timestamp\n",
        "                        e.latency = datetime.now().timestamp() - e.timestamp  # Event's timestamp was set during its creation\n",
        "                        self.consensus_events.append(e)\n",
        "                        self.first_event_of_not_consensus_index[e.owner] = self.first_event_of_not_consensus_index.get(e.owner, 0) + 1\n",
        "\n",
        "        # Bring all consensus events ordered\n",
        "        self.consensus_events.sort(key=lambda x: (x.round_received, x.consensus_timestamp))\n",
        "\n",
        "    # see: If we can reach target using downward edges only, we can see it.\n",
        "    def see(self, current: Event, target: Event) -> bool:\n",
        "        dp_map = self.see_dp_memory.setdefault(current.signature, {})\n",
        "        if target.signature in dp_map:\n",
        "            return dp_map[target.signature]\n",
        "\n",
        "        if (current.signature == target.signature or\n",
        "            (current.round > target.round and current.owner == target.owner)):\n",
        "            dp_map[target.signature] = True\n",
        "            return True\n",
        "        if (current.round < target.round or\n",
        "            (current.is_witness and current.round == target.round) or\n",
        "            is_initial(current)):\n",
        "            dp_map[target.signature] = False\n",
        "            return False\n",
        "\n",
        "        # Recursive check\n",
        "        self_parent = self.events.get(current.self_parent_hash)\n",
        "        other_parent = self.events.get(current.other_parent_hash)\n",
        "        result = False\n",
        "        if self_parent:\n",
        "            result = self.see(self_parent, target)\n",
        "        if not result and other_parent:\n",
        "            result = self.see(other_parent, target)\n",
        "        dp_map[target.signature] = result\n",
        "        return result\n",
        "\n",
        "    # stronglySee: If we see the target, and we go through 2n/3 different nodes as we do that,\n",
        "    # we say we strongly see that target. This function is used for choosing the famous witness\n",
        "    def strongly_see(self, current: Event, target: Event) -> bool:\n",
        "        latest_ancestors = self.get_latest_ancestor_from_all_nodes(current, target.round)\n",
        "        count = sum(1 for ancestor in latest_ancestors.values() if self.see(ancestor, target))\n",
        "        return float(count) > (2.0 * len(self.hashgraph) / 3.0)\n",
        "\n",
        "    # get_latest_ancestor_from_all_nodes: Do breadth first search to find the latest ancestor that event e can see on every node\n",
        "    def get_latest_ancestor_from_all_nodes(self, e: Event, min_round: int) -> Dict[str, Event]:\n",
        "        latest_ancestors: Dict[str, Event] = {}\n",
        "        if not is_initial(e):\n",
        "            queue: List[Event] = [e]\n",
        "            while queue:\n",
        "                current_event = queue.pop(0)\n",
        "\n",
        "                current_ancestor = latest_ancestors.get(current_event.owner)\n",
        "                if not current_ancestor:\n",
        "                    latest_ancestors[current_event.owner] = current_event\n",
        "                elif (current_event.round > current_ancestor.round and\n",
        "                      current_event.owner == current_ancestor.owner):\n",
        "                    latest_ancestors[current_event.owner] = current_event\n",
        "                elif (current_event.round >= current_ancestor.round and\n",
        "                      self.see(current_event, current_ancestor)):\n",
        "                    latest_ancestors[current_event.owner] = current_event\n",
        "\n",
        "                if not is_initial(current_event):\n",
        "                    self_parent = self.events.get(current_event.self_parent_hash)\n",
        "                    other_parent = self.events.get(current_event.other_parent_hash)\n",
        "                    if self_parent and self_parent.round >= min_round:\n",
        "                        queue.append(self_parent)\n",
        "                    if other_parent and other_parent.round >= min_round:\n",
        "                        queue.append(other_parent)\n",
        "        return latest_ancestors\n",
        "\n",
        "    # find_witnesses_of_a_round: Find witnesses of round r, which is the first event with round r in every node\n",
        "    def find_witnesses_of_a_round(self, r: int) -> Dict[str, Event]:\n",
        "        witnesses = {}\n",
        "        for addr in self.hashgraph:\n",
        "            witness = self.witnesses.get(addr, {}).get(r)\n",
        "            if witness:\n",
        "                witnesses[addr] = witness\n",
        "        return witnesses\n",
        "\n",
        "    # GenerateTransactions: Generates an arbitrary amount of random transactions\n",
        "    def generate_transactions(self, count: int, max_amount: float, min_amount: float, peer_addresses: List[str]) -> List[Transaction]:\n",
        "        transactions = []\n",
        "        for _ in range(count):\n",
        "            transactions.append(Transaction(\n",
        "                sender_address=self.address,\n",
        "                payload={'hello': 'world'},\n",
        "            ))\n",
        "        return transactions\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "# is_initial: Returns true if given event is an initial event, false otherwise.\n",
        "def is_initial(e: Event) -> bool:\n",
        "    return e.self_parent_hash == \"\" or e.other_parent_hash == \"\"\n",
        "\n",
        "# Handle error: In Python, exceptions are used instead of panic.\n",
        "def handle_error(e: Exception) -> None:\n",
        "    if e:\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecxfZiR7VkYf"
      },
      "outputs": [],
      "source": [
        "EVALUATION_MODE = True  # Flag to indicate evaluation mode. Performance metrics are measured and printed in evaluation mode.\n",
        "GOSSIP_WAIT_TIME = 0.1  # Seconds between each random gossip\n",
        "CONNECTION_ATTEMPT_DELAY_TIME = 0.1  # Seconds between each connection attempt\n",
        "PRINT_PER_MRPC_CALL = 20  # After this many RPC calls, print out evaluations\n",
        "\n",
        "class SimpleThreadedXMLRPCServer(socketserver.ThreadingMixIn, rpcserver.SimpleXMLRPCServer):\n",
        "    pass\n",
        "\n",
        "class DLedger:\n",
        "    \"\"\"\n",
        "    DLedger: Class for a member of the distributed ledger\n",
        "    \"\"\"\n",
        "    def __init__(self, node: HgNode, my_address: str, peer_addresses: List[str], peer_address_map: Dict[str, str]):\n",
        "        self.node = node\n",
        "        self.my_address = my_address\n",
        "        self.peer_addresses = peer_addresses\n",
        "        self.peer_address_map = peer_address_map\n",
        "        self.peer_client_map = {}\n",
        "        self.on_election_done = None\n",
        "        self.global_model_ready = None\n",
        "        self.shard = None\n",
        "        self.is_aggregator = False\n",
        "        self.shard_aggregators = {}\n",
        "        self.client_locks = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def new_dledger_from_peers(port: str, peer_address_map: Dict[str, str], my_dataset: List[float], model_arch) -> 'DLedger':\n",
        "        local_ip_address = get_local_address()\n",
        "        my_address = f\"{local_ip_address}:{port}\"\n",
        "        # Assert that your own address is in the peers file\n",
        "        if my_address not in peer_address_map:\n",
        "            raise Exception(f\"Peers file does not include my address: {my_address}\")\n",
        "\n",
        "        # Copy peer addresses to a list for random access during gossip\n",
        "        peer_addresses = [addr for addr in peer_address_map if addr != my_address]\n",
        "\n",
        "        # Setup the Hashgraph\n",
        "        signature_uuid = uuid.uuid4()\n",
        "        signature = str(signature_uuid)\n",
        "\n",
        "        initial_hashgraph = {addr: [] for addr in peer_address_map}  # We should not know any event other than our own event at the start\n",
        "        initial_event = Event(\n",
        "            owner=my_address,\n",
        "            signature=signature,\n",
        "            self_parent_hash=\"\",\n",
        "            other_parent_hash=\"\",\n",
        "            timestamp=time.time(),\n",
        "            transactions=[],\n",
        "            round=1,\n",
        "            is_witness=True,  # True because the initial event is the first event of its round\n",
        "            is_famous=False,\n",
        "            is_fame_decided=False,\n",
        "            round_received=0,\n",
        "            consensus_timestamp=0\n",
        "        )\n",
        "        initial_hashgraph[my_address].append(initial_event)\n",
        "        my_node = HgNode(initial_hashgraph, my_address, my_dataset, model_arch)\n",
        "\n",
        "        for addr in my_node.hashgraph:\n",
        "            my_node.witnesses[addr] = {}\n",
        "            my_node.first_event_of_not_consensus_index[addr] = 0  # Index 0 for the initial event\n",
        "\n",
        "        my_node.witnesses[initial_event.owner][1] = initial_event\n",
        "        my_node.events[initial_event.signature] = initial_event\n",
        "        my_node.first_round_of_fame_undecided[initial_event.owner] = 1\n",
        "\n",
        "        # Setup the server\n",
        "        server = SimpleThreadedXMLRPCServer((my_address.split(\":\")[0], int(my_address.split(\":\")[1])), allow_none=True, logRequests=False)\n",
        "        server.register_instance(my_node)\n",
        "        threading.Thread(target=listen_for_rpc_connections, args=(server,), daemon=True).start()\n",
        "\n",
        "        return DLedger(\n",
        "            node=my_node,\n",
        "            my_address=my_address,\n",
        "            peer_addresses=peer_addresses,\n",
        "            peer_address_map=peer_address_map\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def new_dledger(port: str, peers_file_path: str, my_dataset: List[float], model_arch) -> 'DLedger':\n",
        "        local_ip_address = get_local_address()\n",
        "        peer_address_map = read_peer_addresses(peers_file_path, local_ip_address)\n",
        "        dl = DLedger.new_dledger_from_peers(port, peer_address_map, my_dataset, model_arch)\n",
        "        dl.node.on_commit_received = dl.try_share_reveal\n",
        "        dl.node.on_reveal_received = dl.try_validate_election\n",
        "        dl.node.on_voting_next_round = dl.try_voting_next_round\n",
        "        dl.on_election_done = None\n",
        "        dl.global_model_ready = None\n",
        "        dl.local_models_all_ready = None\n",
        "        dl.shard_models_all_ready = None\n",
        "        dl.global_models_all_ready = None\n",
        "        return dl\n",
        "\n",
        "    def start(self):\n",
        "        threading.Thread(target=gossip_routine, args=(self.node, self.peer_addresses), daemon=True).start()\n",
        "\n",
        "    def perform_transaction(self, payload: Dict):\n",
        "        with self.node.lock:\n",
        "            transaction = Transaction(\n",
        "                sender_address=self.my_address,\n",
        "                payload=payload,\n",
        "            )\n",
        "            self.node.transaction_buffer.append(transaction)\n",
        "\n",
        "    def wait_for_peers(self):\n",
        "        peer_available = [False] * len(self.peer_addresses)\n",
        "        remaining_peers = len(self.peer_addresses)\n",
        "        with print_lock:\n",
        "            print(\"waiting...\\n\")\n",
        "        while remaining_peers > 0:\n",
        "            for index, has_already_responded in enumerate(peer_available):\n",
        "                if has_already_responded:\n",
        "                    continue\n",
        "                try:\n",
        "                    proxy = xmlrpc.client.ServerProxy(f\"http://{self.peer_addresses[index]}\", allow_none=True)\n",
        "                    self.peer_client_map[self.peer_addresses[index]] = proxy\n",
        "                    self.client_locks[self.peer_addresses[index]] = threading.RLock()\n",
        "                    proxy.ping()  # Assuming there's a ping method\n",
        "                    peer_available[index] = True\n",
        "                    remaining_peers -= 1\n",
        "                except Exception as err:\n",
        "                    with print_lock:\n",
        "                        print(err)\n",
        "                    time.sleep(CONNECTION_ATTEMPT_DELAY_TIME)\n",
        "\n",
        "    def try_validate_election(self):\n",
        "      if len(self.node.elector.voting.reveals) == NODE_COUNT:\n",
        "        if not self.node.elector.voting.verify_voting():\n",
        "          raise 'voting validation failed'\n",
        "        self.node.elector.store_voting()\n",
        "        self.node.voting_round_dones[self.my_address] = True\n",
        "        def send_req(addr):\n",
        "          nodes[addr].node.voting_done(self.my_address)\n",
        "          # with xmlrpc.client.ServerProxy(f\"http://{addr}\", allow_none=True) as proxy:\n",
        "          #   proxy.voting_done(self.my_address)\n",
        "        for addr in self.peer_addresses:\n",
        "          send_req(addr)\n",
        "        self.try_voting_next_round()\n",
        "\n",
        "    def try_share_reveal(self):\n",
        "      if len(self.node.elector.voting.commits) == NODE_COUNT:\n",
        "        self.node.elector.voting.reveal(self.my_address, self.node.elector.reveal())\n",
        "        def send_req(addr):\n",
        "          nodes[addr].node.voting_reveal(self.my_address, self.node.elector.reveal())\n",
        "          # with xmlrpc.client.ServerProxy(f\"http://{addr}\", allow_none=True) as proxy:\n",
        "          #   proxy.voting_reveal(self.my_address, self.node.elector.reveal())\n",
        "        for addr in self.peer_addresses:\n",
        "          send_req(addr)\n",
        "      self.try_validate_election()\n",
        "\n",
        "    def try_voting_next_round(self):\n",
        "      if len(self.node.voting_round_dones) == NODE_COUNT:\n",
        "        self.node.voting_round_dones = {}\n",
        "        if len(self.node.elector.sorting) < NODE_COUNT:\n",
        "          self.node.elector.start_voting()\n",
        "          self.share_my_commit()\n",
        "        else:\n",
        "          self.node.shared_order = self.node.elector.sorting\n",
        "          self.node.elector.sorting = []\n",
        "          self.on_election_done()\n",
        "\n",
        "    def start_election(self):\n",
        "      self.node.shared_order = []\n",
        "      self.node.elector.start_election()\n",
        "      with print_lock:\n",
        "        print(\"election started\")\n",
        "      self.share_my_commit()\n",
        "\n",
        "    def start_voting(self):\n",
        "      self.node.elector.start_voting()\n",
        "\n",
        "    def share_my_commit(self):\n",
        "      self.node.elector.voting.commit(self.my_address, self.node.elector.commit())\n",
        "      def send_req(addr):\n",
        "        nodes[addr].node.voting_commit(self.my_address, self.node.elector.commit())\n",
        "        # with xmlrpc.client.ServerProxy(f\"http://{addr}\", allow_none=True) as proxy:\n",
        "        #   proxy.voting_commit(self.my_address, self.node.elector.commit())\n",
        "      for addr in self.peer_addresses:\n",
        "        send_req(addr)\n",
        "      self.try_share_reveal()\n",
        "\n",
        "    def share_initial_weights(self):\n",
        "      self.node.learner.prepare_initial_share()\n",
        "      self.perform_transaction({\n",
        "          'type': 'initial_model_share',\n",
        "          'creator_address': self.my_address,\n",
        "          'weights': self.node.learner.initial_share\n",
        "      })\n",
        "      t = threading.Thread(target=check_for_all_shares, args=(self.my_address, self))\n",
        "      t.start()\n",
        "\n",
        "    def local_model_to_hashgraph(self):\n",
        "      self.perform_transaction({\n",
        "          'type': 'local_model',\n",
        "          'creator_address': self.my_address,\n",
        "          'weights': self.node.learner.model\n",
        "      })\n",
        "      t = threading.Thread(target=check_for_all_local_models, args=(self.my_address, self))\n",
        "      t.start()\n",
        "\n",
        "    def find_my_shard(self):\n",
        "      self.shard = int(math.floor(self.node.shared_order.index(self.my_address) / SHARD_SIZE))\n",
        "      self.is_aggregator = (self.node.shared_order.index(self.my_address) % SHARD_SIZE) < AGGREGATOR_COUNT\n",
        "      self.shard_aggregators = {}\n",
        "      if self.is_aggregator:\n",
        "        self.shard_aggregators[self.my_address] = self.shard\n",
        "      for addr in self.peer_addresses:\n",
        "        s = int(math.floor(self.node.shared_order.index(addr) / SHARD_SIZE))\n",
        "        ig = (self.node.shared_order.index(addr) % SHARD_SIZE) < AGGREGATOR_COUNT\n",
        "        if ig:\n",
        "          self.shard_aggregators[addr] = s\n",
        "\n",
        "    def check_and_start_shard_aggregation(self):\n",
        "      try:\n",
        "        if self.is_aggregator:\n",
        "          list_of_events = list(self.node.events.values())\n",
        "          list_of_events.sort(key=lambda x: x.timestamp)\n",
        "          shard_local_models = []\n",
        "          for event in list_of_events:\n",
        "            for trx in event.transactions:\n",
        "              if ('type' in trx.payload) and (trx.payload['type'] == 'local_model'):\n",
        "                s = int(math.floor(self.node.shared_order.index(trx.payload['creator_address']) / SHARD_SIZE))\n",
        "                if s == self.shard:\n",
        "                  shard_local_models.append(trx.payload['weights'])\n",
        "          layers = []\n",
        "          for i in range(0, len(shard_local_models[0])):\n",
        "            sublayers = []\n",
        "            for j in range(0, len(shard_local_models[0][i])):\n",
        "              sublayers.append(np.mean(np.array([model[i][j] for model in shard_local_models]), axis=0))\n",
        "            layers.append(sublayers)\n",
        "          shard_mean_model = layers\n",
        "          self.perform_transaction({\n",
        "            'type': 'shard_model',\n",
        "            'creator_address': self.my_address,\n",
        "            'weights': shard_mean_model,\n",
        "            'shard': self.shard\n",
        "          })\n",
        "        t = threading.Thread(target=check_for_all_shard_models, args=(self.my_address, self))\n",
        "        t.start()\n",
        "      except Exception as e:\n",
        "        with print_lock:\n",
        "          print(traceback.format_exc())\n",
        "        raise e\n",
        "\n",
        "    def check_and_start_global_aggregation(self):\n",
        "      try:\n",
        "        is_global_aggregator = self.node.shared_order.index(self.my_address) < AGGREGATOR_COUNT\n",
        "        if is_global_aggregator:\n",
        "          list_of_events = list(self.node.events.values())\n",
        "          list_of_events.sort(key=lambda x: x.timestamp)\n",
        "          shard_models = {}\n",
        "          for s in self.shard_aggregators.values():\n",
        "            if s not in shard_models:\n",
        "              shard_models[s] = []\n",
        "          for event in list_of_events:\n",
        "            for trx in event.transactions:\n",
        "              if ('type' in trx.payload) and (trx.payload['type'] == 'shard_model'):\n",
        "                s = trx.payload['shard']\n",
        "                shard_models[s].append(trx.payload['weights'])\n",
        "          shard_chosen_models = []\n",
        "          for s in shard_models.keys():\n",
        "            groups = {}\n",
        "            counter = 0\n",
        "            for w in shard_models[s]:\n",
        "              if len(groups) == 0:\n",
        "                groups[str(counter)] = [w]\n",
        "                counter += 1\n",
        "              else:\n",
        "                found = False\n",
        "                for g_key in groups.keys():\n",
        "                  if compare_weights(groups[g_key][0], w):\n",
        "                    groups[g_key].append(w)\n",
        "                    found = True\n",
        "                    break\n",
        "                if not found:\n",
        "                  groups[str(counter)] = [w]\n",
        "                  counter += 1\n",
        "            if len(groups.keys()) > 1:\n",
        "              raise \"attack !\"\n",
        "            max_vote_key = None\n",
        "            for g_key in groups.keys():\n",
        "              if max_vote_key == None:\n",
        "                max_vote_key = g_key\n",
        "              elif len(groups[g_key]) > len(groups[max_vote_key]):\n",
        "                max_vote_key = g_key\n",
        "            shard_chosen_models.append(groups[max_vote_key][0])\n",
        "          layers = []\n",
        "          for i in range(0, len(shard_chosen_models[0])):\n",
        "            sublayers = []\n",
        "            for j in range(0, len(shard_chosen_models[0][i])):\n",
        "              sublayers.append(np.mean(np.array([model[i][j] for model in shard_chosen_models]), axis=0))\n",
        "            layers.append(sublayers)\n",
        "          global_mean_model = layers\n",
        "          self.perform_transaction({\n",
        "            'type': 'global_model',\n",
        "            'creator_address': self.my_address,\n",
        "            'weights': global_mean_model\n",
        "          })\n",
        "      except Exception as e:\n",
        "        with print_lock:\n",
        "          print(traceback.format_exc())\n",
        "        raise e\n",
        "      t = threading.Thread(target=check_for_all_global_models, args=(self.my_address, self))\n",
        "      t.start()\n",
        "\n",
        "    def aggerate_my_final_model(self):\n",
        "      try:\n",
        "        list_of_events = list(self.node.events.values())\n",
        "        list_of_events.sort(key=lambda x: x.timestamp)\n",
        "        global_models = []\n",
        "        for event in list_of_events:\n",
        "          for trx in event.transactions:\n",
        "            if ('type' in trx.payload) and (trx.payload['type'] == 'global_model'):\n",
        "              global_models.append(trx.payload['weights'])\n",
        "        final_chosen_model = None\n",
        "        groups = {}\n",
        "        counter = 0\n",
        "        for w in global_models:\n",
        "          if len(groups) == 0:\n",
        "            groups[str(counter)] = [w]\n",
        "            counter += 1\n",
        "          else:\n",
        "            found = False\n",
        "            for g_key in groups.keys():\n",
        "              if compare_weights(groups[g_key][0], w):\n",
        "                groups[g_key].append(w)\n",
        "                found = True\n",
        "                break\n",
        "            if not found:\n",
        "              groups[str(counter)] = [w]\n",
        "              counter += 1\n",
        "        if len(groups.keys()) > 1:\n",
        "          raise \"attack !\"\n",
        "        max_vote_key = None\n",
        "        for g_key in groups.keys():\n",
        "          if max_vote_key == None:\n",
        "            max_vote_key = g_key\n",
        "          elif len(groups[g_key]) > len(groups[max_vote_key]):\n",
        "            max_vote_key = g_key\n",
        "        final_chosen_model = groups[max_vote_key][0]\n",
        "        self.node.learner.commit_final_model(final_chosen_model)\n",
        "      except Exception as e:\n",
        "        with print_lock:\n",
        "          print(traceback.format_exc())\n",
        "        raise e\n",
        "\n",
        "def compare_weights(w1, w2):\n",
        "  for i in range(0, len(w1)):\n",
        "    for j in range(0, len(w1[i])):\n",
        "      if not (w1[i][j] == w2[i][j]).all():\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "def check_for_all_global_models(address: str, ledger: DLedger):\n",
        "  with print_lock:\n",
        "    print(\"checking for other global models as \" + address + \"\\n\")\n",
        "  while True:\n",
        "    time.sleep(1)\n",
        "    list_of_events = list(ledger.node.events.values())\n",
        "    list_of_events.sort(key=lambda x: x.timestamp)\n",
        "    all_shared_nodes = {}\n",
        "    for event in list_of_events:\n",
        "      for trx in event.transactions:\n",
        "        if ('type' in trx.payload) and (trx.payload['type'] == 'global_model'):\n",
        "          all_shared_nodes[trx.payload['creator_address']] = True\n",
        "    if len(all_shared_nodes) == AGGREGATOR_COUNT:\n",
        "      break\n",
        "  ledger.global_models_all_ready()\n",
        "\n",
        "def check_for_all_shard_models(address: str, ledger: DLedger):\n",
        "  with print_lock:\n",
        "    print(\"checking for other shard models as \" + address + \"\\n\")\n",
        "  while True:\n",
        "    time.sleep(1)\n",
        "    list_of_events = list(ledger.node.events.values())\n",
        "    list_of_events.sort(key=lambda x: x.timestamp)\n",
        "    all_shared_nodes = {}\n",
        "    for event in list_of_events:\n",
        "      for trx in event.transactions:\n",
        "        if ('type' in trx.payload) and (trx.payload['type'] == 'shard_model'):\n",
        "          all_shared_nodes[trx.payload['creator_address']] = True\n",
        "    print(address + ' ' + str(len(all_shared_nodes)))\n",
        "    if len(all_shared_nodes) == math.ceil(AGGREGATOR_COUNT * (NODE_COUNT / SHARD_SIZE)):\n",
        "      break\n",
        "  ledger.shard_models_all_ready()\n",
        "\n",
        "def check_for_all_local_models(address: str, ledger: DLedger):\n",
        "  with print_lock:\n",
        "    print(\"checking for other local models as \" + address + \"\\n\")\n",
        "  while True:\n",
        "    time.sleep(1)\n",
        "    list_of_events = list(ledger.node.events.values())\n",
        "    list_of_events.sort(key=lambda x: x.timestamp)\n",
        "    all_shared_nodes = {}\n",
        "    for event in list_of_events:\n",
        "      for trx in event.transactions:\n",
        "        if ('type' in trx.payload) and (trx.payload['type'] == 'local_model'):\n",
        "          all_shared_nodes[trx.payload['creator_address']] = True\n",
        "    if len(all_shared_nodes) == NODE_COUNT:\n",
        "      break\n",
        "  ledger.local_models_all_ready()\n",
        "\n",
        "def check_for_all_shares(address: str, ledger: DLedger):\n",
        "  with print_lock:\n",
        "    print(\"checking for other shares as \" + address + \"\\n\")\n",
        "  while True:\n",
        "    time.sleep(1)\n",
        "    list_of_events = list(ledger.node.events.values())\n",
        "    list_of_events.sort(key=lambda x: x.timestamp)\n",
        "    all_shared_nodes = {}\n",
        "    for event in list_of_events:\n",
        "      for trx in event.transactions:\n",
        "        if ('type' in trx.payload) and (trx.payload['type'] == 'initial_model_share'):\n",
        "          all_shared_nodes[trx.payload['creator_address']] = True\n",
        "    if len(all_shared_nodes) == NODE_COUNT:\n",
        "      break\n",
        "  for event in list_of_events:\n",
        "    for trx in event.transactions:\n",
        "      if ('type' in trx.payload) and (trx.payload['type'] == 'initial_model_share'):\n",
        "        ledger.node.learner.commit_other_initial_shares(trx.payload['creator_address'], trx.payload['weights'])\n",
        "  ledger.node.learner.prepare_initial_model(ledger.node.shared_order)\n",
        "  ledger.global_model_ready()\n",
        "\n",
        "def create_evaluation_string(node: HgNode, rpc_calls_so_far: int, start_of_gossip: float) -> str:\n",
        "    # How long have I been gossipping\n",
        "    gossip_duration = time.time() - start_of_gossip\n",
        "\n",
        "    # What is the average latency among them\n",
        "    latency_total = sum(event.latency for event in node.consensus_events)\n",
        "    latency_avg = (latency_total / len(node.consensus_events)) if node.consensus_events else 0\n",
        "\n",
        "    # How many events are there in total\n",
        "    num_events = sum(len(events) for events in node.hashgraph.values())\n",
        "\n",
        "    eval_str = (\n",
        "        \"\\n#### EVAL ####\"\n",
        "        f\"\\n\\tGossip Runtime: {gossip_duration:.5f} (sec)\"\n",
        "        f\"\\n\\tGossip Count: {rpc_calls_so_far}\"\n",
        "        f\"\\n\\tAvg. Gossip/sec: {rpc_calls_so_far / gossip_duration if gossip_duration > 0 else 0:.5f}\"\n",
        "        f\"\\n\\tAvg. Latency: {latency_avg:.5f} (sec)\"\n",
        "        f\"\\n\\tNum. of Events: {num_events}\"\n",
        "        f\"\\n\\tNum. of Consensus Events: {len(node.consensus_events)}\"\n",
        "        \"\\n#### EVAL ####\\n\"\n",
        "    )\n",
        "    return eval_str\n",
        "\n",
        "def gossip_routine(node: HgNode, peer_addresses: List[str]):\n",
        "\n",
        "    peer_client_map = {}\n",
        "    for addr in peer_addresses:\n",
        "        peer_client_map[addr] = xmlrpc.client.ServerProxy(f\"http://{addr}\", allow_none=True)\n",
        "\n",
        "    def close_connections():\n",
        "        for client in peer_client_map.values():\n",
        "            try:\n",
        "                client.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    try:\n",
        "        c = 0\n",
        "        start_of_gossip = time.time()\n",
        "        event_evaluation_milestone_reached = False\n",
        "        while True:\n",
        "            # Choose a peer\n",
        "            random_peer_addr = random.choice(peer_addresses)\n",
        "            random_peer_connection = peer_client_map.get(random_peer_addr)\n",
        "            if not random_peer_connection:\n",
        "                continue\n",
        "\n",
        "            # Calculate how many events I know\n",
        "            known_event_nums = {addr: len(events) for addr, events in node.hashgraph.items()}\n",
        "\n",
        "            if EVALUATION_MODE:\n",
        "                num_events = sum(known_event_nums.values())\n",
        "                if num_events >= 5000 and not event_evaluation_milestone_reached:\n",
        "                    event_evaluation_milestone_reached = True\n",
        "                    eval_string = create_evaluation_string(node, c, start_of_gossip)\n",
        "                    with print_lock:\n",
        "                        print(eval_string)\n",
        "\n",
        "            # Ask the chosen peer how many events they do not know but I know\n",
        "            try:\n",
        "                num_events_to_send = nodes[random_peer_addr].node.get_number_of_missing_events(known_event_nums)\n",
        "            except Exception as e:\n",
        "                handle_error(e)\n",
        "                continue\n",
        "\n",
        "            any_event = False\n",
        "            for addr, num_missing in num_events_to_send.items():\n",
        "              if num_missing > 0:\n",
        "                any_event = True\n",
        "                break\n",
        "\n",
        "            # Send the missing events\n",
        "            missing_events = {}\n",
        "            for addr, num_missing in num_events_to_send.items():\n",
        "                if num_missing > 0:\n",
        "                    total_num_events = known_event_nums.get(addr, 0)\n",
        "                    start_index = total_num_events - num_missing\n",
        "                    missing_events[addr] = node.hashgraph[addr][start_index:]\n",
        "\n",
        "            # Wrap the missing events in a struct for RPC, attach my own address here\n",
        "            sync_events_dto = SyncEventsDTO(\n",
        "                sender_address=node.address,\n",
        "                missing_events=missing_events\n",
        "            )\n",
        "\n",
        "            if EVALUATION_MODE and c % PRINT_PER_MRPC_CALL == 0:\n",
        "                eval_string = create_evaluation_string(node, c, start_of_gossip)\n",
        "                if any_event:\n",
        "                    with print_lock:\n",
        "                        print(eval_string)\n",
        "\n",
        "            # Sync all events\n",
        "            try:\n",
        "                nodes[random_peer_addr].node.sync_all_events(sync_events_dto)\n",
        "            except Exception as e:\n",
        "                handle_error(e)\n",
        "                continue\n",
        "\n",
        "            c += 1\n",
        "            time.sleep(GOSSIP_WAIT_TIME)\n",
        "    finally:\n",
        "        close_connections()\n",
        "\n",
        "def read_peer_addresses(path: str, local_ip_addr: str) -> Dict[str, str]:\n",
        "    peers = {}\n",
        "    try:\n",
        "        with open(path, 'r') as file:\n",
        "            for line in file:\n",
        "                addr_name = line.strip().split(\" \")\n",
        "                addr = addr_name[0].replace(\"localhost\", local_ip_addr, 1)\n",
        "                peers[addr] = addr_name[1]\n",
        "    except Exception as e:\n",
        "        handle_error(e)\n",
        "    return peers\n",
        "\n",
        "def listen_for_rpc_connections(server: xmlrpc.server.SimpleXMLRPCServer):\n",
        "    server.serve_forever()\n",
        "\n",
        "def get_local_address() -> str:\n",
        "    try:\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
        "            # This doesn't have to be reachable\n",
        "            s.connect((\"8.8.8.8\", 80))\n",
        "            return s.getsockname()[0]\n",
        "    except Exception as e:\n",
        "        handle_error(e)\n",
        "\n",
        "def handle_error(e: Exception):\n",
        "    if e is not None:\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FGNiuPCmKjH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "file_url = 'https://www.kaggle.com/api/v1/datasets/download/uciml/human-activity-recognition-with-smartphones'\n",
        "\n",
        "r = requests.get(file_url, stream = True)\n",
        "\n",
        "with open(\"/content/ds.zip\", \"wb\") as file:\n",
        "    for block in r.iter_content(chunk_size = 1024):\n",
        "         if block:\n",
        "             file.write(block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTdw6g82mW4p",
        "outputId": "8ebdc4ad-8e9d-4752-ba00-fb2606c978dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ds.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/ds.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbTJF-QBmeSU"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Step 1: Read data from Excel file\n",
        "csv_file_path = \"/content/train.csv\"\n",
        "test_csv_file_path = \"/content/train.csv\"\n",
        "\n",
        "data = pd.read_csv(csv_file_path)\n",
        "test_data = pd.read_csv(test_csv_file_path)\n",
        "\n",
        "\n",
        "#  Convert activities to numeric values\n",
        "activity_mapping = {\n",
        "    'WALKING': 1,\n",
        "    'WALKING_UPSTAIRS': 2,\n",
        "    'WALKING_DOWNSTAIRS': 3,\n",
        "    'SITTING': 4,\n",
        "    'STANDING': 5,\n",
        "    'LAYING': 6\n",
        "}\n",
        "\n",
        "data['Activity_numeric'] = data['Activity'].map(activity_mapping)\n",
        "test_data['Activity_numeric'] = test_data['Activity'].map(activity_mapping)\n",
        "\n",
        "# central database\n",
        "data_central = data[data['subject'] < 26]\n",
        "# Dictionary to store separate databases\n",
        "databases = {}\n",
        "\n",
        "# Filter rows where subject number is between 26 and 30\n",
        "for subject_number in range(26, 31):\n",
        "    databases[subject_number] = data[data['subject'] == subject_number]\n",
        "\n",
        "def preprocess_data(dataset, num_classes):\n",
        "    # Assuming the dataset is a pandas DataFrame\n",
        "    X = dataset.drop(columns=['Activity', 'Activity_numeric']).to_numpy()\n",
        "    X_train = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "    y = dataset['Activity_numeric'].to_numpy()\n",
        "    y_one_hot = to_categorical(y - 1, num_classes=num_classes)  # y - 1 to convert labels from 1-6 to 0-5\n",
        "    return X_train, y_one_hot\n",
        "\n",
        "# Central data\n",
        "X, y_one_hot = preprocess_data(data_central, num_classes=6)\n",
        "# Test data\n",
        "test_X, test_y_one_hot = preprocess_data(test_data, num_classes=6)\n",
        "\n",
        "# Federated data\n",
        "preprocessed_federated_data = {}  # Dictionary to store preprocessed data\n",
        "\n",
        "for dataset_index in range(26, 31):\n",
        "    temp_X, temp_y_one_hot = preprocess_data(databases[dataset_index], num_classes=6)\n",
        "\n",
        "    # Initialize KFold\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_index = 0\n",
        "    for train_index, valid_index in kf.split(temp_X):\n",
        "        X_train, X_valid = temp_X[train_index], temp_X[valid_index]\n",
        "        y_train, y_valid = temp_y_one_hot[train_index], temp_y_one_hot[valid_index]\n",
        "\n",
        "        preprocessed_federated_data[f'{dataset_index}_fold_{fold_index}'] = {\n",
        "            'X_train': X_train,\n",
        "            'X_valid': X_valid,\n",
        "            'y_train': y_train,\n",
        "            'y_valid': y_valid\n",
        "        }\n",
        "        fold_index += 1\n",
        "\n",
        "# Now preprocessed_federated_data contains the 5-fold splits for each dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtViiGMYmpAD"
      },
      "outputs": [],
      "source": [
        "feature_shape = X.shape[1]\n",
        "classes = 6\n",
        "lr =0.01\n",
        "\n",
        "# Define your custom central model architecture\n",
        "def build_model(feature_shape, classes=6):\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    # Convolutional layers\n",
        "    model.add(Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(feature_shape)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    # Flatten layer\n",
        "    model.add(Flatten())\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    # Output layer\n",
        "    model.add(Dense(classes, activation='softmax')) # num_classes should be adjusted based on your classification task\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykjmKqn6WCqo"
      },
      "outputs": [],
      "source": [
        "def main(index, name, port):\n",
        "\n",
        "    with print_lock:\n",
        "        print(name)\n",
        "\n",
        "    model_arch = build_model(X.shape[1:],classes)\n",
        "\n",
        "    client_id = FIRST_CLIENT + index\n",
        "\n",
        "    print(list(preprocessed_federated_data.keys()))\n",
        "\n",
        "    x_local, y_local =   preprocessed_federated_data[f'{client_id}_fold_{FOLD_INDEX}']['X_train'], preprocessed_federated_data[f'{client_id}_fold_{FOLD_INDEX}']['y_train']\n",
        "    x_local_valid, y_local_valid =  preprocessed_federated_data[f'{client_id}_fold_{FOLD_INDEX}']['X_valid'], preprocessed_federated_data[f'{client_id}_fold_{FOLD_INDEX}']['y_valid']\n",
        "\n",
        "    distributed_ledger = DLedger.new_dledger(port, \"peers.txt\", [x_local, y_local, x_local_valid, y_local_valid], model_arch)\n",
        "    nodes[distributed_ledger.my_address] = distributed_ledger\n",
        "\n",
        "    distributed_ledger.wait_for_peers()\n",
        "    with print_lock:\n",
        "        print(f\"I am online at {distributed_ledger.my_address} and all peers are available.\")\n",
        "\n",
        "    distributed_ledger.start()\n",
        "\n",
        "    def on_sharing_initial_model_election_done():\n",
        "      with print_lock:\n",
        "        print(\"initialization election is finished\\n\")\n",
        "      distributed_ledger.global_model_ready = on_initial_global_model_ready\n",
        "      distributed_ledger.share_initial_weights()\n",
        "\n",
        "    def on_initial_global_model_ready():\n",
        "      with print_lock:\n",
        "        print(\"training global model as \" + distributed_ledger.my_address + \"\\n\")\n",
        "      distributed_ledger.node.learner.initial_train(X, y_one_hot)\n",
        "      distributed_ledger.node.learner.train()\n",
        "      loss, accuracy = distributed_ledger.node.learner.model_arch.evaluate(test_X, test_y_one_hot)\n",
        "      with print_lock:\n",
        "        print(\"trained local model is ready as \" + distributed_ledger.my_address + \"\\n\")\n",
        "      distributed_ledger.local_models_all_ready = on_all_local_models_ready\n",
        "      distributed_ledger.local_model_to_hashgraph()\n",
        "\n",
        "    def on_all_local_models_ready():\n",
        "      with print_lock:\n",
        "        print(\"all local models are ready on the hashgraph\\n\")\n",
        "      distributed_ledger.on_election_done = on_sharding_election_done\n",
        "      distributed_ledger.start_election()\n",
        "\n",
        "    def on_sharding_election_done():\n",
        "      with print_lock:\n",
        "        print(\"sharding election is finished\\n\")\n",
        "      distributed_ledger.find_my_shard()\n",
        "      with print_lock:\n",
        "        print(\"shard of \" + distributed_ledger.my_address + \" is \" + str(distributed_ledger.shard) + \"\\n\")\n",
        "      distributed_ledger.shard_models_all_ready = on_all_shard_models_ready\n",
        "      distributed_ledger.check_and_start_shard_aggregation()\n",
        "\n",
        "    def on_all_shard_models_ready():\n",
        "      with print_lock:\n",
        "        print(\"aggregation done on aggregator as \" + distributed_ledger.my_address + \"\\n\")\n",
        "        print(\"starting next election for global aggregators\\n\")\n",
        "      distributed_ledger.on_election_done = on_globalling_election_done\n",
        "      distributed_ledger.start_election()\n",
        "\n",
        "    def on_globalling_election_done():\n",
        "      with print_lock:\n",
        "        print(\"globalling election is finished\\n\")\n",
        "        print(distributed_ledger.node.shared_order)\n",
        "      distributed_ledger.global_models_all_ready = on_all_global_models_ready\n",
        "      distributed_ledger.check_and_start_global_aggregation()\n",
        "      with print_lock:\n",
        "        print(\"global aggregation done on global aggregator as \" + distributed_ledger.my_address + \"\\n\")\n",
        "\n",
        "    def on_all_global_models_ready():\n",
        "      with print_lock:\n",
        "        print(\"all global models are ready on hashgraph\\n\")\n",
        "      distributed_ledger.aggerate_my_final_model()\n",
        "      with print_lock:\n",
        "        print(\"final global model is ready\\n\")\n",
        "      distributed_ledger.node.learner.prepare_for_transfer()\n",
        "      distributed_ledger.node.learner.train()\n",
        "      loss, accuracy = distributed_ledger.node.learner.model_arch.evaluate(test_X, test_y_one_hot)\n",
        "      print( distributed_ledger.my_address + ', accuracy: '  + str(accuracy))\n",
        "\n",
        "    distributed_ledger.on_election_done = on_sharing_initial_model_election_done\n",
        "    distributed_ledger.start_election()\n",
        "\n",
        "    def print_global_models():\n",
        "      while True:\n",
        "        time.sleep(5)\n",
        "        with print_lock:\n",
        "          for event in distributed_ledger.node.events.values():\n",
        "            for trx in event.transactions:\n",
        "              if ('type' in trx.payload) and (trx.payload['type'] == 'global_model'):\n",
        "                print(trx.payload['weights'])\n",
        "\n",
        "    def print_periodically():\n",
        "      while True:\n",
        "        time.sleep(5)\n",
        "        with print_lock:\n",
        "          print(\"transactions in \" + distributed_ledger.my_address + \": -----------------------------\\n\")\n",
        "          for k, v in distributed_ledger.node.events.items():\n",
        "            print(k + ' ' + str(v.timestamp) + ' ' + json.dumps(list(map(lambda b: b.payload, v.transactions))))\n",
        "          print(\"------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "    # threading.Thread(target=print_global_models).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TxWJJ084ZYCG",
        "outputId": "6148df6a-208d-4512-c787-7cd017a80bec"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172.28.0.12:8081\n",
            "172.28.0.12:8082\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172.28.0.12:8083\n",
            "172.28.0.12:8084\n",
            "172.28.0.12:8085\n",
            "['26_fold_0', '26_fold_1', '26_fold_2', '26_fold_3', '26_fold_4', '27_fold_0', '27_fold_1', '27_fold_2', '27_fold_3', '27_fold_4', '28_fold_0', '28_fold_1', '28_fold_2', '28_fold_3', '28_fold_4', '29_fold_0', '29_fold_1', '29_fold_2', '29_fold_3', '29_fold_4', '30_fold_0', '30_fold_1', '30_fold_2', '30_fold_3', '30_fold_4']['26_fold_0', '26_fold_1', '26_fold_2', '26_fold_3', '26_fold_4', '27_fold_0', '27_fold_1', '27_fold_2', '27_fold_3', '27_fold_4', '28_fold_0', '28_fold_1', '28_fold_2', '28_fold_3', '28_fold_4', '29_fold_0', '29_fold_1', '29_fold_2', '29_fold_3', '29_fold_4', '30_fold_0', '30_fold_1', '30_fold_2', '30_fold_3', '30_fold_4']\n",
            "['26_fold_0', '26_fold_1', '26_fold_2', '26_fold_3', '26_fold_4', '27_fold_0', '27_fold_1', '27_fold_2', '27_fold_3', '27_fold_4', '28_fold_0', '28_fold_1', '28_fold_2', '28_fold_3', '28_fold_4', '29_fold_0', '29_fold_1', '29_fold_2', '29_fold_3', '29_fold_4', '30_fold_0', '30_fold_1', '30_fold_2', '30_fold_3', '30_fold_4']\n",
            "['26_fold_0', '26_fold_1', '26_fold_2', '26_fold_3', '26_fold_4', '27_fold_0', '27_fold_1', '27_fold_2', '27_fold_3', '27_fold_4', '28_fold_0', '28_fold_1', '28_fold_2', '28_fold_3', '28_fold_4', '29_fold_0', '29_fold_1', '29_fold_2', '29_fold_3', '29_fold_4', '30_fold_0', '30_fold_1', '30_fold_2', '30_fold_3', '30_fold_4']\n",
            "\n",
            "waiting...\n",
            "\n",
            "waiting...\n",
            "\n",
            "[Errno 111] Connection refused\n",
            "waiting...\n",
            "\n",
            "[Errno 111] Connection refused\n",
            "[Errno 111] Connection refused\n",
            "waiting...\n",
            "\n",
            "[Errno 111] Connection refused\n",
            "['26_fold_0', '26_fold_1', '26_fold_2', '26_fold_3', '26_fold_4', '27_fold_0', '27_fold_1', '27_fold_2', '27_fold_3', '27_fold_4', '28_fold_0', '28_fold_1', '28_fold_2', '28_fold_3', '28_fold_4', '29_fold_0', '29_fold_1', '29_fold_2', '29_fold_3', '29_fold_4', '30_fold_0', '30_fold_1', '30_fold_2', '30_fold_3', '30_fold_4']\n",
            "waiting...\n",
            "\n",
            "I am online at 172.28.0.12:8081 and all peers are available.\n",
            "\n",
            "#### EVAL ####\n",
            "\tGossip Runtime: 0.00004 (sec)\n",
            "\tGossip Count: 0\n",
            "\tAvg. Gossip/sec: 0.00000\n",
            "\tAvg. Latency: 0.00000 (sec)\n",
            "\tNum. of Events: 1\n",
            "\tNum. of Consensus Events: 0\n",
            "#### EVAL ####\n",
            "\n",
            "election started\n",
            "I am online at 172.28.0.12:8085 and all peers are available.\n",
            "I am online at 172.28.0.12:8083 and all peers are available.\n",
            "\n",
            "#### EVAL ####\n",
            "\tGossip Runtime: 0.00002 (sec)\n",
            "\tGossip Count: 0\n",
            "\tAvg. Gossip/sec: 0.00000\n",
            "\tAvg. Latency: 0.00000 (sec)\n",
            "\tNum. of Events: 1\n",
            "\tNum. of Consensus Events: 0\n",
            "#### EVAL ####\n",
            "\n",
            "election started\n",
            "\n",
            "#### EVAL ####\n",
            "\tGossip Runtime: 0.00002 (sec)\n",
            "\tGossip Count: 0\n",
            "\tAvg. Gossip/sec: 0.00000\n",
            "\tAvg. Latency: 0.00000 (sec)\n",
            "\tNum. of Events: 2\n",
            "\tNum. of Consensus Events: 0\n",
            "#### EVAL ####\n",
            "\n",
            "election started\n",
            "I am online at 172.28.0.12:8082 and all peers are available.\n",
            "\n",
            "#### EVAL ####\n",
            "\tGossip Runtime: 0.00002 (sec)\n",
            "\tGossip Count: 0\n",
            "\tAvg. Gossip/sec: 0.00000\n",
            "\tAvg. Latency: 0.00000 (sec)\n",
            "\tNum. of Events: 2\n",
            "\tNum. of Consensus Events: 0\n",
            "#### EVAL ####\n",
            "\n",
            "election started\n",
            "I am online at 172.28.0.12:8084 and all peers are available.\n",
            "\n",
            "#### EVAL ####\n",
            "\tGossip Runtime: 0.00002 (sec)\n",
            "\tGossip Count: 0\n",
            "\tAvg. Gossip/sec: 0.00000\n",
            "\tAvg. Latency: 0.00000 (sec)\n",
            "\tNum. of Events: 2\n",
            "\tNum. of Consensus Events: 0\n",
            "#### EVAL ####\n",
            "\n",
            "election started\n",
            "initialization election is finished\n",
            "\n",
            "checking for other shares as 172.28.0.12:8081\n",
            "\n",
            "initialization election is finished\n",
            "\n",
            "checking for other shares as 172.28.0.12:8082\n",
            "\n",
            "initialization election is finished\n",
            "\n",
            "checking for other shares as 172.28.0.12:8083\n",
            "\n",
            "initialization election is finished\n",
            "\n",
            "checking for other shares as 172.28.0.12:8085\n",
            "\n",
            "initialization election is finished\n",
            "\n",
            "checking for other shares as 172.28.0.12:8084\n",
            "\n",
            "training global model as 172.28.0.12:8085\n",
            "\n",
            "training global model as 172.28.0.12:8083\n",
            "\n",
            "training global model as 172.28.0.12:8084\n",
            "\n",
            "training global model as 172.28.0.12:8081\n",
            "\n",
            "training global model as 172.28.0.12:8082\n",
            "\n",
            "Epoch 1/80\n",
            "Epoch 1/80\n",
            "Epoch 1/80\n",
            "Epoch 1/80\n",
            "Epoch 1/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 196ms/step - accuracy: 0.2143 - loss: 1.7532 - val_accuracy: 0.3640 - val_loss: 1.6072\n",
            "Epoch 2/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 207ms/step - accuracy: 0.2189 - loss: 1.7538 - val_accuracy: 0.3883 - val_loss: 1.6182\n",
            "Epoch 2/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 249ms/step - accuracy: 0.2159 - loss: 1.7526 - val_accuracy: 0.3743 - val_loss: 1.6385\n",
            "Epoch 2/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3816 - loss: 1.5206 - val_accuracy: 0.3354 - val_loss: 1.3212\n",
            "Epoch 3/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 271ms/step - accuracy: 0.2108 - loss: 1.7543 - val_accuracy: 0.3019 - val_loss: 1.6203\n",
            "Epoch 2/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4190 - loss: 1.2181 - val_accuracy: 0.3664 - val_loss: 1.1746\n",
            "Epoch 4/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4515 - loss: 1.0736 - val_accuracy: 0.5295 - val_loss: 1.0842\n",
            "Epoch 5/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5074 - loss: 1.0144 - val_accuracy: 0.4047 - val_loss: 1.0940\n",
            "Epoch 6/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 226ms/step - accuracy: 0.2061 - loss: 1.7567 - val_accuracy: 0.3414 - val_loss: 1.6222\n",
            "Epoch 2/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4982 - loss: 0.9920 - val_accuracy: 0.4845 - val_loss: 1.0121\n",
            "Epoch 7/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5450 - loss: 0.9382 - val_accuracy: 0.4753 - val_loss: 1.0506\n",
            "Epoch 8/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5752 - loss: 0.8877 - val_accuracy: 0.4096 - val_loss: 1.0525\n",
            "Epoch 9/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.3838 - loss: 1.5477 - val_accuracy: 0.3506 - val_loss: 1.3094\n",
            "Epoch 3/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5410 - loss: 0.8866 - val_accuracy: 0.6719 - val_loss: 1.0022\n",
            "Epoch 10/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4311 - loss: 1.2248 - val_accuracy: 0.4900 - val_loss: 1.1216\n",
            "Epoch 4/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5950 - loss: 0.8247 - val_accuracy: 0.4991 - val_loss: 1.0386\n",
            "Epoch 11/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4747 - loss: 1.0749 - val_accuracy: 0.4467 - val_loss: 1.0756\n",
            "Epoch 5/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5865 - loss: 0.8122 - val_accuracy: 0.5125 - val_loss: 0.9626\n",
            "Epoch 12/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5109 - loss: 1.0137 - val_accuracy: 0.4443 - val_loss: 1.0508\n",
            "Epoch 6/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5824 - loss: 0.8110 - val_accuracy: 0.5247 - val_loss: 0.9481\n",
            "Epoch 13/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6324 - loss: 0.7472 - val_accuracy: 0.5393 - val_loss: 0.9922\n",
            "Epoch 14/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6081 - loss: 0.7760 - val_accuracy: 0.6817 - val_loss: 0.9402\n",
            "Epoch 15/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.4848 - loss: 1.0149 - val_accuracy: 0.4230 - val_loss: 1.0325\n",
            "Epoch 7/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5392 - loss: 0.9385 - val_accuracy: 0.5027 - val_loss: 1.0125\n",
            "Epoch 8/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6734 - loss: 0.7142 - val_accuracy: 0.5873 - val_loss: 0.9398\n",
            "Epoch 16/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6521 - loss: 0.7243 - val_accuracy: 0.6598 - val_loss: 0.8836\n",
            "Epoch 17/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5280 - loss: 0.9159 - val_accuracy: 0.4967 - val_loss: 0.9887\n",
            "Epoch 9/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6348 - loss: 0.7192 - val_accuracy: 0.5740 - val_loss: 0.9997\n",
            "Epoch 18/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5522 - loss: 0.8835 - val_accuracy: 0.4668 - val_loss: 0.9923\n",
            "Epoch 10/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6512 - loss: 0.7038 - val_accuracy: 0.6744 - val_loss: 0.8576\n",
            "Epoch 19/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5625 - loss: 0.8690 - val_accuracy: 0.4693 - val_loss: 0.9805\n",
            "Epoch 11/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6690 - loss: 0.6635 - val_accuracy: 0.6433 - val_loss: 0.8438\n",
            "Epoch 20/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5947 - loss: 0.8245 - val_accuracy: 0.5466 - val_loss: 0.9444\n",
            "Epoch 12/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6889 - loss: 0.6517 - val_accuracy: 0.5526 - val_loss: 0.9514\n",
            "Epoch 21/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5984 - loss: 0.7809 - val_accuracy: 0.5015 - val_loss: 0.9654\n",
            "Epoch 13/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.3699 - loss: 1.5303 - val_accuracy: 0.3646 - val_loss: 1.3075\n",
            "Epoch 3/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 31ms/step - accuracy: 0.3831 - loss: 1.5478 - val_accuracy: 0.4066 - val_loss: 1.3112\n",
            "Epoch 3/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6833 - loss: 0.6787 - val_accuracy: 0.6446 - val_loss: 0.8270\n",
            "Epoch 22/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - accuracy: 0.3811 - loss: 1.5724 - val_accuracy: 0.3518 - val_loss: 1.3930\n",
            "Epoch 3/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4170 - loss: 1.2113 - val_accuracy: 0.4218 - val_loss: 1.1192\n",
            "\u001b[1m 3/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6276 - loss: 0.6641Epoch 4/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.4326 - loss: 1.2919 - val_accuracy: 0.4187 - val_loss: 1.1660\n",
            "\u001b[1m13/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5013 - loss: 1.0966Epoch 4/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.6288 - loss: 0.7548 - val_accuracy: 0.5764 - val_loss: 0.9214\n",
            "Epoch 14/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.4158 - loss: 1.2343 - val_accuracy: 0.4066 - val_loss: 1.2148\n",
            "Epoch 4/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.4760 - loss: 1.0880 - val_accuracy: 0.4041 - val_loss: 1.0763\n",
            "Epoch 5/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6477 - loss: 0.6704 - val_accuracy: 0.6963 - val_loss: 0.7694\n",
            "Epoch 23/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.4839 - loss: 1.0995 - val_accuracy: 0.4607 - val_loss: 1.0934\n",
            "\u001b[1m57/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6553 - loss: 0.7395Epoch 5/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.4779 - loss: 1.0967 - val_accuracy: 0.4011 - val_loss: 1.1259\n",
            "Epoch 5/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6547 - loss: 0.7400 - val_accuracy: 0.6080 - val_loss: 0.8937\n",
            "Epoch 15/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4702 - loss: 1.0519 - val_accuracy: 0.4547 - val_loss: 1.0447\n",
            "Epoch 6/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6847 - loss: 0.6193 - val_accuracy: 0.6439 - val_loss: 0.8772\n",
            "Epoch 24/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.4944 - loss: 1.0274 - val_accuracy: 0.4327 - val_loss: 1.0571\n",
            "Epoch 6/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.4656 - loss: 1.0482 - val_accuracy: 0.4790 - val_loss: 1.0517\n",
            "Epoch 6/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6254 - loss: 0.7385 - val_accuracy: 0.6464 - val_loss: 0.9140\n",
            "Epoch 16/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5089 - loss: 0.9894 - val_accuracy: 0.4175 - val_loss: 1.0780\n",
            "Epoch 7/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5225 - loss: 0.9766 - val_accuracy: 0.4559 - val_loss: 1.0450\n",
            "Epoch 7/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.7282 - loss: 0.5997 - val_accuracy: 0.5752 - val_loss: 0.8492\n",
            "\u001b[1m37/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5432 - loss: 0.9343Epoch 25/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.5117 - loss: 0.9947 - val_accuracy: 0.4693 - val_loss: 1.0220\n",
            "Epoch 7/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.5149 - loss: 0.9552 - val_accuracy: 0.4559 - val_loss: 1.1170\n",
            "Epoch 8/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step - accuracy: 0.6483 - loss: 0.7145 - val_accuracy: 0.5460 - val_loss: 0.9170\n",
            "Epoch 17/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5426 - loss: 0.9326 - val_accuracy: 0.3774 - val_loss: 1.0799\n",
            "Epoch 8/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5363 - loss: 0.9231 - val_accuracy: 0.5813 - val_loss: 1.0692\n",
            "\u001b[1m 8/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4807 - loss: 0.9734 Epoch 9/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5300 - loss: 0.9373 - val_accuracy: 0.5180 - val_loss: 0.9832\n",
            "Epoch 9/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5845 - loss: 0.8487 - val_accuracy: 0.5094 - val_loss: 0.9557\n",
            "Epoch 10/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5679 - loss: 0.8832 - val_accuracy: 0.4960 - val_loss: 1.1290\n",
            "Epoch 10/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.7369 - loss: 0.5835 - val_accuracy: 0.7024 - val_loss: 0.8801\n",
            "Epoch 26/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.6627 - loss: 0.7033 - val_accuracy: 0.5624 - val_loss: 0.9696\n",
            "Epoch 18/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.5145 - loss: 0.9975 - val_accuracy: 0.4510 - val_loss: 1.0968\n",
            "Epoch 8/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.5647 - loss: 0.8379 - val_accuracy: 0.5953 - val_loss: 1.0042\n",
            "Epoch 11/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.5608 - loss: 0.8673 - val_accuracy: 0.4827 - val_loss: 1.0230\n",
            "Epoch 11/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.7607 - loss: 0.5397 - val_accuracy: 0.7803 - val_loss: 0.6842\n",
            "\u001b[1m31/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5514 - loss: 0.8997Epoch 27/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.6823 - loss: 0.6773 - val_accuracy: 0.6859 - val_loss: 0.8623\n",
            "Epoch 19/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.5506 - loss: 0.9005 - val_accuracy: 0.5198 - val_loss: 0.9876\n",
            "\u001b[1m 7/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8366 - loss: 0.4258Epoch 9/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.5834 - loss: 0.8375 - val_accuracy: 0.6153 - val_loss: 0.9669\n",
            "\u001b[1m27/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5776 - loss: 0.8877Epoch 12/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.6143 - loss: 0.7971 - val_accuracy: 0.5344 - val_loss: 0.9740\n",
            "Epoch 12/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7866 - loss: 0.4873 - val_accuracy: 0.7529 - val_loss: 0.6699\n",
            "Epoch 28/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.5673 - loss: 0.8900 - val_accuracy: 0.5271 - val_loss: 0.9943\n",
            "Epoch 10/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.6708 - loss: 0.6708 - val_accuracy: 0.5557 - val_loss: 1.0117\n",
            "\u001b[1m 3/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7656 - loss: 0.5075  Epoch 20/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6055 - loss: 0.7841 - val_accuracy: 0.5435 - val_loss: 1.1044\n",
            "Epoch 13/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6056 - loss: 0.7926 - val_accuracy: 0.5593 - val_loss: 1.0186\n",
            "\u001b[1m53/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5356 - loss: 0.9134Epoch 13/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7845 - loss: 0.4601 - val_accuracy: 0.7809 - val_loss: 0.6551\n",
            "Epoch 29/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5405 - loss: 0.9068 - val_accuracy: 0.5654 - val_loss: 0.9892\n",
            "Epoch 11/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6164 - loss: 0.7863 - val_accuracy: 0.5435 - val_loss: 0.9850\n",
            "Epoch 14/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.7927 - loss: 0.4696 - val_accuracy: 0.8180 - val_loss: 0.5992\n",
            "Epoch 30/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.6138 - loss: 0.7632 - val_accuracy: 0.4979 - val_loss: 0.9603\n",
            "Epoch 14/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.5608 - loss: 0.8484 - val_accuracy: 0.4072 - val_loss: 1.3413\n",
            "Epoch 12/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - accuracy: 0.6888 - loss: 0.6595 - val_accuracy: 0.6013 - val_loss: 0.8873\n",
            "Epoch 21/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.7999 - loss: 0.4310 - val_accuracy: 0.7407 - val_loss: 0.6150\n",
            "Epoch 31/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.6382 - loss: 0.7578 - val_accuracy: 0.6945 - val_loss: 0.8859\n",
            "Epoch 15/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.6317 - loss: 0.7349 - val_accuracy: 0.6275 - val_loss: 0.9350\n",
            "\u001b[1m39/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6947 - loss: 0.6774Epoch 15/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.5690 - loss: 0.8481 - val_accuracy: 0.6123 - val_loss: 0.9414\n",
            "Epoch 13/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6865 - loss: 0.6802 - val_accuracy: 0.6549 - val_loss: 0.8229\n",
            "Epoch 22/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6733 - loss: 0.7207 - val_accuracy: 0.4948 - val_loss: 1.0597\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8262 - loss: 0.3852 - val_accuracy: 0.5180 - val_loss: 1.2134\n",
            "Epoch 16/80\n",
            "Epoch 32/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6030 - loss: 0.7857 - val_accuracy: 0.5344 - val_loss: 0.9834\n",
            "Epoch 14/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8026 - loss: 0.5789 - val_accuracy: 0.7352 - val_loss: 0.6425\n",
            "Epoch 33/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.6180 - loss: 0.7631 - val_accuracy: 0.5192 - val_loss: 1.0197\n",
            "Epoch 17/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6992 - loss: 0.6312 - val_accuracy: 0.4942 - val_loss: 1.0963\n",
            "\u001b[1m51/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6195 - loss: 0.7551Epoch 23/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.6414 - loss: 0.7421 - val_accuracy: 0.5271 - val_loss: 0.9452\n",
            "Epoch 16/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6204 - loss: 0.7541 - val_accuracy: 0.4741 - val_loss: 1.0415\n",
            "\u001b[1m18/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6215 - loss: 0.7357Epoch 15/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.8342 - loss: 0.3507 - val_accuracy: 0.7182 - val_loss: 0.6435\n",
            "Epoch 34/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.6388 - loss: 0.7326 - val_accuracy: 0.5380 - val_loss: 0.9721\n",
            "Epoch 17/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.6967 - loss: 0.6528 - val_accuracy: 0.5210 - val_loss: 0.9934\n",
            "Epoch 24/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6291 - loss: 0.7345 - val_accuracy: 0.6026 - val_loss: 0.9086\n",
            "\u001b[1m10/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7960 - loss: 0.3785Epoch 16/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.6643 - loss: 0.6993 - val_accuracy: 0.4595 - val_loss: 1.1564\n",
            "Epoch 18/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6704 - loss: 0.7156 - val_accuracy: 0.7164 - val_loss: 0.8460\n",
            "Epoch 18/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8286 - loss: 0.3386 - val_accuracy: 0.7955 - val_loss: 0.5730\n",
            "Epoch 35/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6158 - loss: 0.7406 - val_accuracy: 0.6026 - val_loss: 0.9318\n",
            "Epoch 17/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6759 - loss: 0.6541 - val_accuracy: 0.5940 - val_loss: 0.9459\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8177 - loss: 0.3583 - val_accuracy: 0.8278 - val_loss: 0.5483\n",
            "Epoch 19/80\n",
            "Epoch 36/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6456 - loss: 0.7222 - val_accuracy: 0.6537 - val_loss: 0.9063\n",
            "Epoch 18/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.6725 - loss: 0.7129 - val_accuracy: 0.5892 - val_loss: 0.8802\n",
            "Epoch 19/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - accuracy: 0.7069 - loss: 0.6053 - val_accuracy: 0.7614 - val_loss: 0.7421\n",
            "Epoch 25/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.8528 - loss: 0.3292 - val_accuracy: 0.8089 - val_loss: 0.5800\n",
            "Epoch 37/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.6551 - loss: 0.6935 - val_accuracy: 0.4863 - val_loss: 0.9881\n",
            "Epoch 20/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.6514 - loss: 0.7114 - val_accuracy: 0.6579 - val_loss: 0.9356\n",
            "Epoch 19/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6878 - loss: 0.6754 - val_accuracy: 0.5441 - val_loss: 0.9229\n",
            "Epoch 20/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.7450 - loss: 0.5656 - val_accuracy: 0.7200 - val_loss: 0.7473\n",
            "\u001b[1m26/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6719 - loss: 0.6795Epoch 26/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.6856 - loss: 0.6347 - val_accuracy: 0.6257 - val_loss: 0.8414\n",
            "\u001b[1m37/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6666 - loss: 0.6816Epoch 21/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6626 - loss: 0.6821 - val_accuracy: 0.6202 - val_loss: 0.9173\n",
            "Epoch 20/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6731 - loss: 0.6693 - val_accuracy: 0.5581 - val_loss: 0.9058\n",
            "Epoch 21/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8422 - loss: 0.3196 - val_accuracy: 0.8107 - val_loss: 0.5852\n",
            "Epoch 38/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.7072 - loss: 0.6144 - val_accuracy: 0.6561 - val_loss: 0.8189\n",
            "Epoch 22/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - accuracy: 0.7587 - loss: 0.5341 - val_accuracy: 0.7389 - val_loss: 0.7324\n",
            "\u001b[1m50/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6895 - loss: 0.6281Epoch 27/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.6358 - loss: 0.7577 - val_accuracy: 0.6713 - val_loss: 0.9568\n",
            "Epoch 21/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6912 - loss: 0.6266 - val_accuracy: 0.6719 - val_loss: 0.8197\n",
            "Epoch 22/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.7081 - loss: 0.6225 - val_accuracy: 0.6738 - val_loss: 0.8360\n",
            "Epoch 23/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8376 - loss: 0.3334 - val_accuracy: 0.8253 - val_loss: 0.5298\n",
            "Epoch 39/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6662 - loss: 0.7254 - val_accuracy: 0.5411 - val_loss: 0.9198\n",
            "\u001b[1m24/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8772 - loss: 0.2874Epoch 22/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.6739 - loss: 0.6699 - val_accuracy: 0.7468 - val_loss: 0.7973\n",
            "Epoch 23/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.6912 - loss: 0.6370 - val_accuracy: 0.6847 - val_loss: 0.8376\n",
            "Epoch 24/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8680 - loss: 0.2906 - val_accuracy: 0.7620 - val_loss: 0.5968\n",
            "\u001b[1m11/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6273 - loss: 0.7096Epoch 40/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.7127 - loss: 0.6380 - val_accuracy: 0.6975 - val_loss: 0.7709\n",
            "Epoch 24/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.8532 - loss: 0.3072 - val_accuracy: 0.7785 - val_loss: 0.5629\n",
            "Epoch 41/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.7747 - loss: 0.5168 - val_accuracy: 0.7614 - val_loss: 0.7261\n",
            "\u001b[1m 8/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7975 - loss: 0.3625 Epoch 28/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.7670 - loss: 0.5323 - val_accuracy: 0.6902 - val_loss: 0.7485\n",
            "Epoch 25/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.6606 - loss: 0.6716 - val_accuracy: 0.6847 - val_loss: 0.8584\n",
            "Epoch 23/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.7444 - loss: 0.5739 - val_accuracy: 0.6354 - val_loss: 0.9615\n",
            "Epoch 25/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8515 - loss: 0.3070 - val_accuracy: 0.7511 - val_loss: 0.5847\n",
            "Epoch 42/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6831 - loss: 0.6707 - val_accuracy: 0.7322 - val_loss: 0.8158\n",
            "Epoch 24/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7146 - loss: 0.5889 - val_accuracy: 0.7267 - val_loss: 0.7553\n",
            "Epoch 26/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.6747 - loss: 0.6604 - val_accuracy: 0.6439 - val_loss: 0.8391\n",
            "Epoch 25/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.7558 - loss: 0.5512 - val_accuracy: 0.4942 - val_loss: 1.0209\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.7507 - loss: 0.5374 - val_accuracy: 0.5526 - val_loss: 1.0626\n",
            "Epoch 26/80\n",
            "Epoch 27/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.7967 - loss: 0.4565 - val_accuracy: 0.7845 - val_loss: 0.6536\n",
            "Epoch 29/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.8603 - loss: 0.2872 - val_accuracy: 0.8205 - val_loss: 0.5544\n",
            "Epoch 43/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8105 - loss: 0.4308 - val_accuracy: 0.7699 - val_loss: 0.6295\n",
            "Epoch 30/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7500 - loss: 0.5488 - val_accuracy: 0.5879 - val_loss: 0.8394\n",
            "\u001b[1m 1/60\u001b[0m \u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.7812 - loss: 0.4414Epoch 27/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7365 - loss: 0.5767 - val_accuracy: 0.7632 - val_loss: 0.6754\n",
            "Epoch 28/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.8682 - loss: 0.2803 - val_accuracy: 0.8357 - val_loss: 0.5218\n",
            "Epoch 44/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.6957 - loss: 0.6274 - val_accuracy: 0.7827 - val_loss: 0.7450\n",
            "Epoch 26/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.8193 - loss: 0.4224 - val_accuracy: 0.8217 - val_loss: 0.5924\n",
            "\u001b[1m57/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7576 - loss: 0.5206Epoch 31/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.7701 - loss: 0.5091 - val_accuracy: 0.7900 - val_loss: 0.6310\n",
            "Epoch 29/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.7576 - loss: 0.5212 - val_accuracy: 0.7200 - val_loss: 0.6536\n",
            "Epoch 28/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.8672 - loss: 0.2889 - val_accuracy: 0.7413 - val_loss: 0.6427\n",
            "Epoch 45/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.7806 - loss: 0.4725 - val_accuracy: 0.7505 - val_loss: 0.6271\n",
            "Epoch 30/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7281 - loss: 0.5946 - val_accuracy: 0.7273 - val_loss: 0.7610\n",
            "Epoch 27/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.8024 - loss: 0.4379 - val_accuracy: 0.7851 - val_loss: 0.6613\n",
            "Epoch 29/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8065 - loss: 0.3914 - val_accuracy: 0.8168 - val_loss: 0.6092\n",
            "\u001b[1m 4/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7077 - loss: 0.5388 Epoch 32/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8510 - loss: 0.2900 - val_accuracy: 0.7547 - val_loss: 0.5864\n",
            "\u001b[1m12/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8542 - loss: 0.4133Epoch 46/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8100 - loss: 0.4628 - val_accuracy: 0.7340 - val_loss: 0.6705\n",
            "Epoch 30/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.7269 - loss: 0.5669 - val_accuracy: 0.5800 - val_loss: 0.8973\n",
            "Epoch 28/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.7965 - loss: 0.4270 - val_accuracy: 0.7383 - val_loss: 0.6567\n",
            "\u001b[1m14/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7736 - loss: 0.4500Epoch 31/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8186 - loss: 0.3613 - val_accuracy: 0.6987 - val_loss: 0.6905\n",
            "\u001b[1m12/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8320 - loss: 0.3599Epoch 33/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.8549 - loss: 0.2871 - val_accuracy: 0.7797 - val_loss: 0.5682\n",
            "Epoch 47/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.7996 - loss: 0.4109 - val_accuracy: 0.8125 - val_loss: 0.5899\n",
            "Epoch 31/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.7318 - loss: 0.5547 - val_accuracy: 0.6732 - val_loss: 0.8309\n",
            "Epoch 29/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8246 - loss: 0.3723 - val_accuracy: 0.6859 - val_loss: 0.8092\n",
            "Epoch 32/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8599 - loss: 0.2901 - val_accuracy: 0.8241 - val_loss: 0.6167\n",
            "Epoch 48/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8164 - loss: 0.4093 - val_accuracy: 0.7699 - val_loss: 0.5960\n",
            "Epoch 32/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8607 - loss: 0.2915 - val_accuracy: 0.7450 - val_loss: 0.5673\n",
            "Epoch 49/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.8092 - loss: 0.3983 - val_accuracy: 0.7754 - val_loss: 0.5933\n",
            "Epoch 34/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.7359 - loss: 0.5809 - val_accuracy: 0.7219 - val_loss: 0.7258\n",
            "Epoch 30/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.8160 - loss: 0.3597 - val_accuracy: 0.7900 - val_loss: 0.5902\n",
            "Epoch 33/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.8136 - loss: 0.3804 - val_accuracy: 0.6726 - val_loss: 0.8573\n",
            "\u001b[1m29/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8596 - loss: 0.3455Epoch 33/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.8363 - loss: 0.3291 - val_accuracy: 0.7821 - val_loss: 0.6140\n",
            "Epoch 35/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.7764 - loss: 0.5059 - val_accuracy: 0.7766 - val_loss: 0.6327\n",
            "\u001b[1m13/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8060 - loss: 0.3807Epoch 31/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.8652 - loss: 0.2719 - val_accuracy: 0.8247 - val_loss: 0.4784\n",
            "Epoch 50/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8490 - loss: 0.3518 - val_accuracy: 0.8180 - val_loss: 0.5772\n",
            "Epoch 34/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.7922 - loss: 0.5120 - val_accuracy: 0.7797 - val_loss: 0.5726\n",
            "\u001b[1m52/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7888 - loss: 0.4584Epoch 34/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8186 - loss: 0.3686 - val_accuracy: 0.8058 - val_loss: 0.5806\n",
            "Epoch 36/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7900 - loss: 0.4563 - val_accuracy: 0.5155 - val_loss: 1.8561\n",
            "\u001b[1m29/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8534 - loss: 0.3083Epoch 32/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8759 - loss: 0.2628 - val_accuracy: 0.8198 - val_loss: 0.4955\n",
            "Epoch 51/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.8492 - loss: 0.3450 - val_accuracy: 0.7377 - val_loss: 0.5755\n",
            "Epoch 35/80\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8488 - loss: 0.3196 - val_accuracy: 0.8113 - val_loss: 0.6057\n",
            "\u001b[1m41/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8419 - loss: 0.3272Epoch 35/80\n",
            "\u001b[1m52/60\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8287 - loss: 0.3330"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-824e903a334a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"172.28.0.12:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ports = [(i + 8081) for i in range(0, NODE_COUNT)]\n",
        "\n",
        "with open('peers.txt', \"w\") as f:\n",
        "  text = \"\"\n",
        "  for port in ports:\n",
        "    text += \"172.28.0.12:\" + str(port) + \" \" + str(port) + \"\\n\"\n",
        "  f.write(text)\n",
        "\n",
        "for index, port in enumerate(ports):\n",
        "  threading.Thread(target=main, args=(index, \"172.28.0.12:\" + str(port), port)).start()\n",
        "\n",
        "time.sleep(60 * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uPyrSyN9_J8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8546e20-280f-41c9-d646-5baa8b2bec7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[93.5, 94.5, 93.05, 92.7, 94.1]\n"
          ]
        }
      ],
      "source": [
        "results = [\n",
        "    [94, 95, 92, 93, 95],\n",
        "    [92, 94, 92, 92, 93],\n",
        "    [92, 95, 94, 94, 93],\n",
        "    [94, 95, 92, 95, 95],\n",
        "    [92, 93, 95, 91, 94],\n",
        "    [95, 95, 93, 93, 95],\n",
        "    [92, 95, 93, 92, 94],\n",
        "    [93, 93, 93, 93, 95],\n",
        "    [94, 95, 92, 90, 94],\n",
        "    [93, 95, 95, 92, 94],\n",
        "    [94, 95, 95, 93, 94],\n",
        "    [94, 94, 93, 93, 94],\n",
        "    [94, 94, 90, 95, 94],\n",
        "    [94, 95, 92, 94, 93],\n",
        "    [94, 95, 96, 92, 94],\n",
        "    [93, 95, 96, 90, 95],\n",
        "    [95, 95, 91, 95, 94],\n",
        "    [93, 94, 91, 92, 93],\n",
        "    [94, 94, 95, 92, 95],\n",
        "    [94, 94, 91, 93, 94]\n",
        "]\n",
        "\n",
        "medians = [0, 0, 0, 0, 0]\n",
        "\n",
        "for j in range(0, len(medians)):\n",
        "  for _, m in enumerate(results):\n",
        "    medians[j] += m[j]\n",
        "  medians[j] /= len(results)\n",
        "\n",
        "print(medians)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}